<!DOCTYPE html>
<html>
<head>
    <title>Factorization Machine</title>
    <script type="text/javascript" src="../js/jquery.min.js"></script>
    <script type="text/javascript" src="../js/highcharts.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link href='../css/shCore.css' rel='stylesheet' type='text/css'></link>
    <link href='../css/shThemeDefault.css' rel='stylesheet' type='text/css'></link>
    <script type="text/javascript" src="../js/shCore.js"></script>
    <script type="text/javascript" src="../js/shBrushJScript.js"></script>
    <script type="text/javascript" src="../js/util.js"></script>
    <script type="text/javascript" src="../js/rnd.js"></script>
    <script type="text/javascript" src="../js/data_set.js"></script>
    <style type="text/css">
    #graph {
        width: 600px;
        height: 600px;
    }
    </style>
    <script type="text/javascript" id="main_script">
    // [MainScriptStarts]
    
    function logit(x) {
      return 1.0 / (1.0 + Math.exp(-x));
    }
    
    var square = function(x) {
      return x * x;
    }

    var FactorizedModel = (function(){
      var w0 = 0;
      var w = [];
      var v = [];
      var num_features = 0;
      var num_factors = 0;
      
      // model_spec = {num_features, number_factors, sigma}
      var init = function(model_spec) {
        num_features = model_spec.num_features;
        num_factors = model_spec.num_factors;
        var sigma = model_spec.sigma;
        
        w0 = 0;
        w = [];
        for (var i = 0; i < num_features; ++i) {
          w.push(0);
        }
        var guassian_rnd = $RND.gaussian_rng(0, sigma);
        v = [];
        for (var i = 0; i < num_features; ++i) {
          var cur_factors = [];
          for (var j = 0; j < num_factors; ++j) {
            cur_factors.push(guassian_rnd());
          }
          v.push(cur_factors);
        }
      }
      
      // \hat y(x) = \omega_0 + \sum{i=1}^n{\omega_i * x_i} 
      //   + 0.5 * \sum_{f=1}^f{ 
           //      (\sum_{i=1}^n{v_{i,f} * x_i)})^2 - sum_{i=1}^n{(v_{i,f}*x_i)^2} } 
      var predict = function(x) {
        var r = w0;
        for (var i = 0; i < num_features; ++i) {
          r += w[i] * x[i];
        }
        for (var f = 0; f < num_factors; ++f) {
          var v_x_sum  = 0;
          var v_x_sum_sq = 0;
          for (var i = 0; i < num_features; ++i) {
            v_x_sum += v[i][f] * x[i];
            v_x_sum_sq += square(v[i][f] * x[i]);
          }
          r += 0.5 * (square(v_x_sum) - v_x_sum_sq);
        }
        return r;
      }

      // train_spec = {cost_diff, learning_rate, regularization}
      var train = function(train_spec, training_set, training_cost) {
        var reg = train_spec.regularization;
        var alpha = train_spec.learning_rate;
        var min_cost_diff = train_spec.cost_diff;
        var prev_cost = Infinity;
             for (;;) {
          var cost = 0.0;
          for (var data_i = 0; data_i < training_set.length; ++data_i) {
            var x = training_set[data_i].x;
            var y = training_set[data_i].y;
            var hat_y = predict(x);
            cost += get_cost(y, hat_y, train_spec.type);
            //console.log(cost / (data_i + 1));
            var gradients = gradient(x, y, hat_y, train_spec.type);
            w0 -= alpha * (2 * reg.lambda_0 * w0 + gradients.grad_w0);
            for (var i = 0; i < num_features; ++i) {
              w[i] -= alpha * (2 * reg.lambda_w * w[i] + gradients.grad_w[i]);
              for (var f = 0; f < num_factors; ++f) {
                v[i][f] -= alpha * (2 * reg.lambda_f * v[i][f] + gradients.grad_v[i][f]);
              }
            }
          }
          if (training_cost !== undefined) {
                 training_cost.push(cost);
          }
          if (Math.abs(prev_cost - cost) < min_cost_diff) {
                  break;
          }
          prev_cost = cost;
        }
      }
      
      var get_cost = function(y, hat_y, cost_type) {
          if (cost_type == 'classification') {
                if (y == 0) {
                     return -Math.log(1 - logit(hat_y));
                } else {
                     return -Math.log(logit(hat_y));
                }
          } else if (cost_type == 'regression') {
              return 0.5 * square(y - hat_y);
          } else {
              return hat_y;
          }
      }
      
      var gradient = function(x, y, hat_y, cost_type) {
          var DcostDhat_y = 0;
          if (cost_type == 'classification') {
              DcostDhat_y = logit(hat_y) - y;
          } else if (cost_type == 'regression') {
              DcostDhat_y = (hat_y - y);
          } else {
              DcostDhat_y = 1.0;
          }
          
          var partial_0 = DcostDhat_y * 1.0; // Dhat_y/Dw0 = 1
          var partial_i = [];
          var partial_i_f = [];
          for (var i = 0; i < num_features; ++i) {
            partial_i.push(DcostDhat_y * x[i]); // Dhat_y/Dw_i = x_i
            
            partial_i_f.push([]);
            for (var f = 0; f < num_factors; ++f) {
              var sum_v_x = 0;
              for (var j = 0; j < num_features; ++j) {
                sum_v_x += v[j][f] * x[j];
              }
              var Dhat_yDv_if = x[i]*( sum_v_x - v[i][f] * x[i]);
              partial_i_f[i].push(DcostDhat_y * Dhat_yDv_if); 
            }
          }
          return {
                grad_w0: partial_0,
                grad_w: partial_i,
                grad_v: partial_i_f,
          };
      }

      var test_gradient = function() {
        var old_w0 = w0;
        var old_w = [];
        for (var i = 0; i < w.length; ++i) {
                old_w.push(w[i]);
        }
        var old_v = [];
        for (var i = 0; i < w.length; ++i) {
          var v_copy = [];
          for (var f = 0; f < num_factors; ++f) {
                 v_copy.push(v[i][f]);
          }
          old_v.push(v_copy);
        }
        var old_num_features = num_features;
        var old_num_factors = num_factors;
        
        w0 = 0.05;
        w = [0.02, 0.03];
        v = [[0.04, 0.05], [0.06, 0.07]];
        num_features = 2;
        num_factors = 2;
             
        var costs = ['identity', 'classification', 'regression'];
        for (var c = 0; c < costs.length; ++c) {
                  var cost_type = costs[c];
                  console.log("Testing for [" + cost_type + "]");
                  var x = [1.24, 2.34];
                  var y = 1.0;
                  var hat_y = predict(x);
                  var grads = gradient(x, y, hat_y, cost_type);
            var epsilon = 0.0000001;
            var delta =   0.00005;
            
                  // check grad_w0
                  w0 = old_w0 + epsilon;
                  var p_cost_w0 = get_cost(y, predict(x), cost_type);
                  w0 = old_w0 - epsilon;
                  var n_cost_w0 = get_cost(y, predict(x), cost_type);
                  w0 = old_w0;
                  var grad_w0 = (p_cost_w0 - n_cost_w0) / (2 * epsilon);
                  var msg = "grad_w0, expected: [" + grad_w0 + "] got [" + grads.grad_w0 +"].";
                  if (Math.abs(grads.grad_w0 - grad_w0) > delta) {
                         console.log("Incorrect: " + msg);
                  } else {
                         console.log("Correct: " + msg);
                  }
                  
                  // check grad_wi
                  for (var i = 0; i < num_features; ++i) {
                         w[i] += epsilon;
                         var p_cost = get_cost(y, predict(x), cost_type);
                         w[i] -= 2 * epsilon;
                         var n_cost = get_cost(y, predict(x), cost_type);
                         var grad_w_i = (p_cost - n_cost) / (2 * epsilon);
                         var msg = "grad_w[" + i + "], expected to be [" + grad_w_i +  "] got [" + grads.grad_w[i] + "].";
                         if (Math.abs(grads.grad_w[i] - grad_w_i) > delta) {
                            console.log("Incorrect: " + msg);
                         } else {
                            console.log("Correct: " + msg);
                         }
                         w[i] += epsilon;
                  }
                  
                  // check grad_v
                  for (var i = 0; i < num_features; ++i) {
                         for (var f = 0; f < num_factors; ++f) {
                            v[i][f] += epsilon;
                            var p_cost = get_cost(y, predict(x), cost_type);
                            v[i][f] -= 2 * epsilon;
                 var n_cost = get_cost(y, predict(x), cost_type);
                 var grad_v_i_f = (p_cost - n_cost) / (2 * epsilon);
                 var msg = "grad_v[" + i +"]["+f+"], expected to be [" + grad_v_i_f + "], got [" + grads.grad_v[i][f]+ "].";
                 if (Math.abs(grads.grad_v[i][f] - grad_v_i_f) > delta) {
                     console.log("Incorrect: " + msg);
                 } else {
                     console.log("Correct: " + msg);
                 }
                 v[i][f] -= epsilon;
                         }
                  }
        }
             
             w0 = old_w0;
             w = old_w;
             v = old_v;
             num_features = old_num_features;
             num_factors = old_num_factors;
      }
      
      var dump = function() {
             var str = 'w0 = ' + w0;
             str += ', w = [';
             for (var i = 0; i < num_features; ++i) {
                    str += w[i] + ', ';
             }
             str += ']';
             str += 'v = [';
             for (var i = 0; i < num_features; ++i) {
                    str += '[';
                    for (var f = 0; f < num_factors; ++f) {
                       str += v[i][f] + ', ';
                    }
                    str += '],';
             }
             str += ']';
             return str;
      }
      
      return {
        Init: init,
        Predict: predict,
        Train: train,
        Test: test_gradient,
        Dump: dump,
      };
    })();
    

    // [MainScriptEnds]
    
    var convert_data_to_sample = function(train_data) {
      var samples = [];
      for (var i = 0; i < train_data.length; ++i) {
        samples.push({
          x: [train_data[i][0], train_data[i][1]],
          y: train_data[i][2],
        });
      }
      return samples;
    }
    
    // Retrieve plot point coordinates for the plot.
    var get_plot_data = function(train_data) {
        var X0 = [];
        var x0_i = 0;
        var X1 = [];
        var x1_i = 0;
        var min_x0 = 1000;
        var max_x0 = 0;
        for (var i = 0; i < train_data.length; ++i) {
            if (train_data[i][0] < min_x0) {
                min_x0 = train_data[i][0];
            }
            if (train_data[i][0] > max_x0) {
                max_x0 = train_data[i][0];
            }
    
            if (train_data[i][2]>0) {
                X0[x0_i] = {
                    x: train_data[i][0],
                    y: train_data[i][1],
                };
                x0_i++;
            } else {
                X1[x1_i] = {
                    x: train_data[i][0],
                    y: train_data[i][1],
                };
                x1_i++;
            }
        };
        return {
            "positives": X0, 
            "negatives": X1,
            "max_x0": max_x0, 
            "min_x0": min_x0,
        };
    };

    var plot_result = function(container, class_result) {
        chart = new Highcharts.Chart({
            chart: {
                renderTo: container,
                type: 'scatter',
                zoomType: 'xy'
            },
            width: "450px",
            height: "450px",
            title: {
                text: 'Classification Result'
            },
            xAxis: {
                title: {
                    enabled: true,
                    text: 'x1'
                },
                startOnTick: true,
                endOnTick: true,
                showLastLabel: true
            },
            yAxis: {
                title: {
                    text: 'x2'
                }
            },
            tooltip: {
                formatter: function() {
                        return ''+
                        this.x +' , '+ this.y;
                }
            },
            legend: {
                layout: 'vertical',
                align: 'left',
                verticalAlign: 'top',
                x: 100,
                y: 70,
                floating: true,
                backgroundColor: '#FFFFFF',
                borderWidth: 1
            },
            plotOptions: {
                scatter: {
                    marker: {
                        radius: 5,
                        states: {
                            hover: {
                                enabled: true,
                                lineColor: 'rgb(100,100,100)'
                            }
                        }
                    },
                    states: {
                        hover: {
                            marker: {
                                enabled: false
                            }
                        }
                    }
                }
            },
            series: [{
                name: 'True Positive',
                color: 'rgba(223, 83, 83, .5)',
                data: class_result.tp,
    
            }, {
                name: 'True Negative',
                color: 'rgba(119, 152, 191, .5)',
                data: class_result.tn,
            }, {
                    name: 'False Positive',
                    data: class_result.fp,
            }, {
                    name: 'False Negative',
                    data: class_result.fn,
            }]
        });
    };
    
    var plot_curve = function(div_id, training_costs) {
        var learn_curve = new Highcharts.Chart({
            chart: {
                renderTo: div_id,
                type: 'line',
            },
            title: {
                text: 'Learning Curve'
            },
            xAxis: {
                title: {
                    enabled: true,
                    text: 'Training Epoch'
                },
            },
            yAxis: {
                title: {
                    text: 'Cost Evaluation'
                }
            },
            series: [{
                name: 'Training Cost',
                data: training_costs,
                marker: {
                    enabled: false,
                },
            }]
        });
    }
    
    Array.prototype.shuffle = function() {
           var tmp = [];
           while (this.length) {
              tmp.push(this.splice(Math.random()* this.length, 1));
           }
           while (tmp.length) {
              this.push(tmp.pop());
           }
           return this;
    }
    
    var process_result = function(FactorizedModel, training_set, 
      train_spec, class_result, prediction_result) {
       for (var i = 0; i < training_set.length; ++i) {
          var cur_instance = [];
          var x = training_set[i].x;
          var y = training_set[i].y;
          var r = FactorizedModel.Predict(x);
          var is_true = y > 0;
          var is_pos = r >= 0.5;
          if (train_spec.type == 'classification') {
                is_pos = logit(r) >= 0.5;
          }
          // console.log("y=" + y + ",r=" + r + ", logit(r)" + logit(r) );
          if (is_true == is_pos) {
            if (is_true) {
              class_result.tp.push([x[0], x[1]]);
            } else {
              class_result.tn.push([x[0], x[1]]);
            }
          } else {
            if (is_true) {
              class_result.fn.push([x[0], x[1]]);
            } else {
              class_result.fp.push([x[0], x[1]]);
            }
          }
          
          if (is_pos) {
            prediction_result.push([x[0], x[1], 1]);
          } else {
            prediction_result.push([x[0], x[1], 0]);
          }
        }
    }

    $(document).ready(function(){
        
        var model_spec = {
          num_features: 2,
          num_factors: 20,
          sigma: 0.0001,
        };
        FactorizedModel.Init(model_spec);
        
        var train_spec = {
          type: 'classification',
          learning_rate: 0.0001,
          cost_diff: 0.00001,
          regularization: {
            lambda_0: 0.0,
            lambda_w: 0.0,
            lambda_f: 0.0,
          }
        };
        DataSet.classification[0].shuffle;
        
        var training_set = convert_data_to_sample(DataSet.classification[0]);
        FactorizedModel.Test();
        
        var training_costs = [];
        FactorizedModel.Train(train_spec, training_set, training_costs);
        
        // dump model weights.
        console.log(FactorizedModel.Dump());
        
        //var prediction_result = [];
        var class_result = {
                  tp:[],
                  tn:[],
                  fp:[],
                  fn:[],
        };
        var prediction_result = [];
        process_result(FactorizedModel, training_set, train_spec, class_result, prediction_result);

        var tp = class_result.tp.length;
        var tn = class_result.tn.length;
        var fp = class_result.fp.length;
        var fn = class_result.fn.length;
        
        var precision = tp*1.0 / (tp + fp);
        var recall = tp*1.0 / (tp + tn);
        var accuracy = (tp + tn)*1.0 / (tp + fp + tn + fn);
        var f1_score = (2.0 * precision*recall / (precision + recall));
        
        $('#stats').html("<table>" +
                  "<tr><td>TP: " + tp + " FN: " + fn +  "</td><td>"+ " FP:" + fp + " TN: " + tn+"</td></tr>" + 
                  "<tr><td>Accuracy:  " + "</td><td>" + accuracy + "</td></tr>" +
                  "<tr><td>Precision: " + "</td><td>"+ precision + "</td></tr>" +
                  "<tr><td>Recall:    " + "</td><td>"+ recall + "</td></tr>" +
                  "<tr><td>F-score:   " + "</td><td>"+ f1_score + "</td></tr>" +
                  "</table>");
        
        plot_result("pred_graph", class_result);
        plot_curve('learning_curve', training_costs);
        
        RenderCodeSection('main_script', 'code');
    });
    </script>
</head>
<body>
    <h1>Factorization Machines</h1>
    <p>by Weidong Liang</p>
    <p>Beijing, 2014.06</p>
    <hr/>
    
    <h2>Introduction</h2>
    <p>The Factorization Machine model has the following form:
    $$
    \hat y (x) = \omega_0 + \sum_{i=1}^{n}{\omega_i x_i} + 
       \sum_{i=1}^{n} \sum_{j=i+1}^{n}{ \lt v_i , v_j \gt x_i x_j }
    $$
    where
    \( \omega_0 \in \mathbb{R} \), \( w \in \mathbb{R}^p \) and \( V \in \mathbb{R}^{p \times k} \).
    Direct evaluation of y using the above requires a time complexity of \( O(k n ^ 2) \).
    </p>
    <p>The model can be efficiently computed using
    $$
    \hat y (x) = \omega_0 + \sum_{i=1}^{n}{\omega_i x_i} +
        \frac{1}{2} \sum_{f=1}^{k}( 
            (\sum_{i=1}^{n}{v_{i, f} x_i } )^2 - \sum_{i=1}^{n}{ v_{i, f}^2 x_i^2 } )
    $$
    which requires only \( O(k n) \).
    </p>
    <hr/>
    
    <h2>Learning FM</h2>
    <p>Two common loss functions and their gradients are:
    <ul>
        <li>Square Loss for regression:
        \( L^{LS}(\hat y, y) = \frac{1}{2} (\hat y - y)^2 \), 
        its derivative is: 
        \( \frac{d L^{LS}(\hat y, y)}{d \hat y} = (\hat y - y) \).
        </li>
        <li>Logit loss for binary classification:
        \( L^{C}(\hat y, y) = - (y*ln(\sigma(\hat y)) + (1-y)*ln(1 - \sigma(\hat y))) \),
        its derivative is:
        \( \frac{L^{C}(\hat y, y)}{d \hat y} = \sigma(\hat y) - y \).
        </li>
    </ul>
    </p>
    
    <h3>Learning FM with Stochastic Gradient Descent</h3>
    <p> Deriving the gradient of \( \hat y (x) \) with respect to \( \omega \), we have:
        $$
         \frac{\partial \hat y}{\partial \omega_0} = 1 
        $$
        $$
         \frac{\partial \hat y}{\partial \omega_i} = x_i
        $$
        $$
         \frac{\partial \hat y}{\partial v_{i, f}} = 
            \frac{\partial }{\partial v_{i, f}} 
                \sum_{f=1}^{k}( 
                    (\sum_{j=1}^{n}{v_{j, f} x_j } )^2 - \sum_{i=1}^{n}{ v_{i, f}^2 x_i^2 } ) =
            x_i (\sum_j^n{v_{j,f} x_j} - v_{i,f} x_i)
        $$
    </p>
    <p>Integerating the above with the gradient descent algorithm of regularized optimization:
    $$
     OptReg(S, \lambda) := argmin_{\Theta} 
        ( \sum_{(x, y) \in S}{l(\hat y(x | \Theta), y) + \sum_{\theta \in \Theta}{\lambda_{\theta} \theta^2 }}  )
    $$
    </p>
    <p>
    procedure SolveOptReg(S, \( \lambda \))
    <ol>
        <li> \( \omega_0 := 0 \) </li>
        <li> \( w := (0, 0, ..., 0) \) </li>
        <li> \( V \sim N(0, \sigma) \) </li>
        <li>repeat until stopping criterion is met
            <ol>
                <li>for \( (x, y) \in S \) do
                    <ol>
                        <li> \( \partial_0 := \frac{\partial}{\partial \omega_0}(l(\hat y(x | \Theta), y )) \)</li>
                        <li> \( \omega_0 := \omega_0 - \alpha (2 \lambda_0 \omega_0 + \partial_0) \) </li>
                        <li> for \( i \in {1, ..., p} \) and \( x_i \ne 0 \) do
                            <ul>
                               <li>\( \partial_i := \frac{\partial}{\partial w_i}(l(\hat y(x | \Theta), y )) \) </li>
                               <li>\( w_i := w_i - \alpha(2 \lambda_w w_i + \partial_i) \)</li>
                               <li> for \( f \in {1, ..., k} \) do
                                   <ul>
                                       <li>\( \partial_{i, f} := \frac{\partial}{\partial v_{i, f}}(l(\hat y(x | \Theta), y )) \)</li>
                                       <li>\( v_{i, f} := v_{i, f} - \alpha(2 \lambda_f v_{i, f} + \partial_{i, f}) \)</li>
                                   </ul>
                               </li>
                             </ul>
                        </li>
                    </ol> 
                </li>
            </ol>
        </li>
    </ol>
    </p>
    
    <h3>Learning FM using SGD with Adaptive Regularization</h3>
    <p>The data set is split into training and validation \( S = S_T \cup S_V \), 
    with \( S_T \) for learning \( \Theta \) and \( S_V \) for tuning \( \lambda \).
    This can be done by alternating between improving \( \Theta \) while \( \lambda \) is fixed,
    and improving \( \lambda \) while \( \Theta \) is fixed; with the first part being the same
    as the above, and we shall derive the second part below:
    </p>
    <p>Instead of evaluating:
    $$
        \lambda^* | \Theta^t := argmin_{\lambda \in R_+} \sum_{(x, y) \in S_V} l(\hat y(x | \Theta^t), y)
    $$
    which, having the right side independent of \( \lambda \), would result in a gradient equals 0;
    the main idea is to make the dependence of \( \hat y \) on \( \lambda \) to be explicit using:
    $$
        \hat y(x | \Theta^t) = w_0^{t+1} + \sum_i^{n}{w_i^{t+1} x_i} + 
            \sum_{i=1}^n \sum_{j=i+1}^n < v_i^{t+1}, v_j^{t+1} > x_i x_j
    $$
    Combining the above with
    $$
        \theta^{t+1} = \theta^t - \alpha (
            \frac{\partial}{\partial \theta^t} l(\hat y (x | \Theta ^ t), y) + 2 \lambda \theta ^ t
            )
    $$
    we have
    $$
        \hat y(x | \Theta^{t+1}) = \{ w_0^t - \alpha (
            \frac{\partial}{\partial \omega_0^t} l(\hat y (x | \Theta ^ t), y) + 2 \lambda \omega_0 ^ t
            ) \} +
            \sum_{i = 1}^n x_i \{ w_i^t - \alpha (
                \frac{\partial}{\partial \omega_i^t} l(\hat y (x | \Theta ^ t), y) + 2 \lambda \omega_i ^ t
                ) \} +
            \sum_{i = 1}^n \sum_{j=i+1}^n \sum_{f=1}^{k}[
                x_i \{
                    v_{i,f}^t - \alpha (
                        \frac{\partial}{\partial v_{i,f}^t} l(\hat y (x | \Theta ^ t), y) + 2 \lambda v_{i,f}^t
                    ) 
                \}
                x_j \{
                    v_{j,f}^t  - \alpha (
                        \frac{\partial}{\partial v_{j,f}^t} l(\hat y (x | \Theta ^ t), y) + 2 \lambda v_{j,f}^t
                    ) 
                \} 
            ] (Eq.*)
    $$
    it follows that we can solve the following:
    $$
       \lambda^* | \Theta^t := argmin_{\lambda \in R_+} \sum_{(x, y) \in S_V} l(\hat y(x | \Theta^{t+1}), y)
    $$
    which answers the question: "What is the best value of \( \lambda \) such that the next update on \( \Theta \)
    generates the smallest error on the validation set?"
    </p>
    <p>
    The SGD-update for \( \lambda \) given a case \( (x,y) \in S_V \) is
    $$
        \lambda^{t+1} = \lambda^t - \alpha \frac{\partial}{\partial \lambda} l(\hat y(x|\Theta^{t+1}), y)
    $$
    </p>
    <p>Similar to the above, we have derivative of the cost function:
    $$
        \frac{\partial}{\partial \lambda}( \hat y(x | \Theta^{t+1}) - y) ^ 2 = 
            2(\hat y(x | \Theta^{t+1}) - y) \frac{\partial}{\partial \lambda}(\hat y(x | \Theta^{t+1}))
    $$
    $$
        \frac{\partial}{\partial \lambda}{-ln \sigma(\hat y(x | \Theta^{t+1}) y)} =
            (\sigma(\hat y(x | \Theta^{t+1}) y) - 1) y \frac{\partial}{\partial \lambda}(\hat y(x | \Theta^{t+1})) 
    $$
    </p>
    <p>Differentiate the above future model equation, with \( \lambda_0, \lambda_w, \lambda_f \) corresponds to
    the regularization coeffficient for \( \omega_0, w_i, v_{i, f} \) respectively,  we have:
    $$
        \frac{\partial}{\partial \lambda_0}(\hat y(x | \Theta^{t+1})) = -2 \alpha \omega_0^t
    $$
    $$
        \frac{\partial}{\partial \lambda_w}(\hat y(x | \Theta^{t+1})) = -2 \alpha \sum_{i=1}^n{w_i^t x_i}
    $$
    $$
        \frac{\partial}{\partial \lambda_f}(\hat y(x | \Theta^{t+1})) = -2 \alpha [
            \sum_{i=1}^n{x_i v_{i,f}^{t+1}} \sum_{j=1}^n{x_j v_{j,f}^t} - \sum_{j=1}^n{x_j^2 v_{j,f}^{t+1} v_{j,f}^t}
        ]
    $$
    which depends both on the current \( v_{i,f}^t \) and future \( v_{i,f}^{t+1} \).
    </p>
    <p>In order to enable faster calculation, instead of calculating \( v_{i,f}^{t+1} \) using \( (x,y) \in S_T \),
    it can be approximated using:
    $$
        \tilde v_{i,f}^{t+1} := v_{i,f}^t - \alpha ( \partial_{v_{i,f}} + 2 \lambda v_{i,f}^t ) \approx v_{i,f}^{t+1}
    $$
    where \( \partial_{v_{i,f}} \) is the following gradient that has been calculated in the previous \( \Theta \) step.
    </p>
    <p>procedure SolveOptAdaptiveReg( \(S_T, S_V \) )
    <ol>
        <li> \( \omega_0 := 0 \) </li>
        <li> \( w := (0, ..., 0) \) </li>
        <li> \( V := N(0, \sigma) \) </li>
        <li> \( \lambda := (0, ..., 0) \) </li>
        <li> \( \partial := (0, .., 0) \) </li>
        <li> repeat till stopping criterion is met
        <ol>
            <li>for \( (x, y) \in S_T \) do</li>
            <ol>
                <li>\( \partial_0 := \frac{\partial}{\partial \omega_0} l(\hat y(x|\Theta), y) \)</li>
                <li>\( \omega_0 := \omega_0 - \alpha (2 \lambda_0 \omega_0 + \partial_0 ) \) </li>
                <li>for \( i \in \{1, ..., p\} \) and \( x_i \ne 0 \) do
                    <ol>
                        <li>\( \partial_i := \frac{\partial}{\partial \omega_i} l(\hat y(x|\Theta), y) \)</li>
                        <li>\( \omega_i := \omega_i - \alpha ( 2 \lambda_w + \partial_i) \)</li>
                        <li>for \( f \in \{1, ..., k\} \) do
                        <ol>
                            <li>\( \partial_{i,f} := \frac{\partial}{\partial v_{i,f}} l(\hat y(x|\Theta), y) \)</li>
                            <li>\( v_{i,f} := v_{i,f} - \alpha (2 \lambda_f v_{i,f} + \partial_{i,f} ) \)</li>
                        </ol>
                        </li>
                    </ol>
                </li>
                <li>\( (x', y') \sim S_V \)</li>
                <li>\( \lambda_0 := max(0, \lambda_0 - \alpha \frac{\partial}{\partial \lambda_0} l(\hat y(x'|\tilde \Theta), y)) \)</li>
                <li>\( \lambda_w := max(0, \lambda_w - \alpha \frac{\partial}{\partial \lambda_w} l(\hat y(x'|\tilde \Theta), y)) \)</li>
                <li>for \( f \in {1, ..., k} \) do
                    <ol>
                    <li>\( \lambda_f := max(0, \lambda_f - \alpha \frac{\partial}{\partial \lambda_f} l(\hat y(x'|\tilde \Theta), y)) \)</li>
                    </ol>
                </li>
            </ol>
        </ol>
        </li>
    </ol>
    </p>
    
    <h3>Learning FM with Alternating Least Square</h3>
    <p></p>
    
    <h3>Learning FM with Markov Chain Monte Carlo</h3>
    <p></p>
    
    <h2>Simulation</h2>
    <p>
    </p>

    <table>
        <tr>
        <td><span id="pred_graph" style="width:500px;height:500px;display:inline;position:flow;z-index:1;"></span></td>
        <td><span id="learning_curve" style="width:500px;height:500px;display:inline;position:flow;z-index:1;"></span></td>
        </tr>
    </table>
    <div id="stats"></div>

    <hr/>
    
    <h2>Algorithm</h2>
    <p>
    </p>
    <hr/>
    
    <h2>Implementation</h2>
    <div id='code'></div>
    <hr/>
    
    <h2>Note</h2>
    <p>
    </p>
    <hr/>
    
    <h2>Reference</h2>
    <ul>
        <li>S. Rendle, 2012, Learning Recommender Systems With Adaptive Regularization.</li>
    </ul>
    <hr/>
    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'weidongtomlgithubio'; // required: replace example with your forum shortname
        // var disqus_url = 'http://weidongtoml.github.io/sampling/index.html';
        // var disqus_developer = 1;
        var disqus_identifier = '';
        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</body>
</html>
    
        