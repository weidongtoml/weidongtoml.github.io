<!DOCTYPE html>
<html>
<head>
    <title>Gaussian Mixture Model</title>
    <script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.0/jquery.min.js"></script>
    <script type="text/javascript" src="http://code.highcharts.com/highcharts.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link href='../css/shCore.css' rel='stylesheet' type='text/css'></link>
    <link href='../css/shThemeDefault.css' rel='stylesheet' type='text/css'></link>
    <script type="text/javascript" src="../js/shCore.js"></script>
    <script type="text/javascript" src="../js/shBrushJScript.js"></script>
    <script type="text/javascript" src="../js/util.js"></script>
    <script type="text/javascript" src="../js/data_set.js"></script>
    <script type="text/javascript" src="../js/sylvester.src.js"></script>
    <script type="text/javascript" src="../js/rnd.js"></script>
    <script type="text/javascript" id="main_script">
    
    var approx_eq = function(a, b, limit) {
        return Math.abs(a-b) <= limit;
    } 
    
    // [MainScriptStarts]
    
    // GMM with two-dimensional observed variables and K dimensional hidden
    // variable.
    var GaussianMixtureModel = function() {
        var K = 0;
        var N = 0;
        var pi = null;
        var mu = null;
        var sigma = null;
        var inv_sigma = null;
        var det_sigma = null;
        var ones_N = null;
        var ones_K = null;
        var zeros_K = null;
        var zeros_N = null;
        
        var repeat_n = function(n, v) {
            var vec = [];
            for (var i = 0; i < n; ++i) {
                vec[i] = v;
            }
            return vec;
        }
        
        var init = function(x_dim, z_dim) {
            N = x_dim;
            K = z_dim;
            ones_K = $V(repeat_n(K, 1.0));
            ones_N = $V(repeat_n(N, 1.0));
            pi = ones_K.multiply(1.0 / K);
            mu = [];
            for (var k = 0; k < K; ++k) {
                mu.push(ones_N.multiply(1.0 / N));
            }
            
            sigma = [];
            for (var k = 0; k < K; ++k) {
                sigma[k] = Matrix.I(N);
            }
            
            zeros_K = $V(repeat_n(K, 0.0));
            zeros_N = $V(repeat_n(N, 0.0));
        }
        
        // p(x) = \sum_z{p(z) p(x|z)} 
        //    = \sum_{k=1}^K{\pi_k N(x|\mu_k, \sigma_k)}
        var probX = function(x) {
            var p_x_given_z = [];
            for (var k = 0; k < K; ++k) {
                p_x_given_z.push(cond_probX(x, mu[k], sigma[k]));
            }
            return pi.dot($V(p_x_given_z));
        }
        
        // p(x | \mu_k, \sigma_k) = N(x | \mu_k, \sigma_k)
        //    = 1/( (2 \pi)^{N/2} *\sqrt(|\sigma|) ) * 
        //        exp{-0.5 * (x - \mu_k)^T \sigma_k^{-1} (x - \mu_k)}
        var cond_probX = function(x, mu_k, sigma_k) {
            var coeff = 1.0 / (2 * Math.pow(Math.PI, N/2.0) * Math.sqrt(sigma_k.det()));
            var x_sub_mu = x.subtract(mu_k);
            var x_sub_mu_T = $M(x_sub_mu.elements).transpose();
            var delta_sq = x_sub_mu_T.multiply(sigma_k.inverse()).multiply(x_sub_mu);
            return coeff * Math.exp(-0.5 * delta_sq.elements[0]);
        }
        
        // iterates between Estep and Mstep until convergence.
        var train = function(X, train_spec) {
            var prev_likelihood = Infinity;
            var counter = 0;

            show_params();
            
            for (;counter < train_spec.max_steps;) {
                console.log("Epoch = " + counter);
                counter++;
                
                var gamma_z = e_step(X);
                m_step(X, gamma_z);
                var cur_likelihood = log_likelihood(X);

                console.log("Likelihood = " + cur_likelihood);
                
                if (Math.abs(cur_likelihood - prev_likelihood) < train_spec.min_increment) {
                    break;
                }
                
                show_params();
                
                prev_likelihood = cur_likelihood;
            }
        }
        
        var show_params = function() {
            console.log("pi = " + pi.elements);
            for (var k = 0; k < K; ++k) {
                   console.log("mu[" + k + "] = " + mu[k].elements);
                   console.log("sigma[" + k + "] = " + sigma[k].elements);
            }
        }
        
        var probZGivenX = function(x) {
            var p_x_given_z = [];
            for (var k = 0; k < K; ++k) {
                p_x_given_z[k] = pi.elements[k] * cond_probX(x, mu[k], sigma[k]);
            }
            var gamma_z_n = $V(p_x_given_z);
            return gamma_z_n.multiply(1.0 / gamma_z_n.dot(ones_N));
        }
        
        // \gamma(z_{nk}) = \pi_k N(x_n|\mu_k, \sigma_k) /
        //    \sum_{j=1}^K{\pi_j N(x_n|\mu_j, \sigma_j)}
        var e_step = function(X) {
            var gamma_z = [];
            for (var n = 0; n < X.length; ++n) {
                gamma_z[n] = probZGivenX(X[n]);
            }
            return gamma_z;
        }
        
        var m_step = function(X, gamma_z) {
            // N_k = \sum_{n=1}^N{\gamma(z_{nk})}
            var N_k = [];
            for (var k = 0; k < K; ++k) {
                N_k[k] = 0.0;
                for (var n = 0; n < gamma_z.length; ++n) {
                    N_k[k] += gamma_z[n].elements[k];
                } 
            }
            
            // \pi_k = N_k / N
            pi = $V(N_k).multiply(1.0 / gamma_z.length);

            for (var k = 0; k < K; ++k) {
                 // \mu_k = \sum_{n=1}^{N}{\gamma(z_{nk}) x_n}/N_k
                var x_dot_gamma_zk = zeros_N.dup();
                for (var n = 0; n < X.length; ++n) {
                    x_dot_gamma_zk = x_dot_gamma_zk.add(X[n].multiply(gamma_z[n].elements[k]));
                }
                mu[k] = x_dot_gamma_zk.multiply(1.0 / N_k[k]);
            }
            
            for (var k = 0; k < K; ++k) {
                var zeros = [];
                for (var k0 = 0; k0 < K; ++k0) {
                    zeros[k0] = [];
                    for (var k1 = 0; k1 < K; ++k1) {
                        zeros[k0][k1] = 0.0;
                    }
                }
                // \sigma_k = \frac{1}_{Nk} * \sum{n=1}^{N}{ \gamma(z_{nk})(x_n - \mu_k)(x_n - \mu_k)^T }
                var acc = $M(zeros);
                for (var n = 0; n < X.length; ++n) {
                    var x_minus_mu = X[n].subtract(mu[k]);
                    var x_minus_mu_t = $M(x_minus_mu.elements).transpose();
                    var prod = $M(x_minus_mu.elements).multiply(x_minus_mu_t);
                    var prod_t = prod.multiply(gamma_z[n].elements[k]);
                    acc = acc.add(prod_t);
                }
                sigma[k] = acc.multiply(1.0 / N_k[k]);
            }
        }
        
        // ln(p(X|\mu, \sigma, \pi)) = \sum_{n=1}^{N}{
        //       ln{\sum_{k=1}^K{\pi_k N(x_n | \mu_k, \sigma_k)}}}
        var log_likelihood = function(X) {
            var likelihood = 0.0;
            for (var n = 0; n < X.length; ++n) {
                var s = 0.0;
                for (var k = 0; k < K; ++k) {
                    s += pi.elements[k] * cond_probX(X[n], mu[k], sigma[k]);
                }
                
                likelihood += Math.log(s);
            }
            return likelihood;
        }
        
        var unit_test = function() {
            // Test Prob(X)
            this.Init(2, 2);
            var prob_x = this.ProbX([1, 1]);
            var exp_prob_x = 1.0 / (2*Math.PI) * Math.exp(-0.5 * 0.5);
            var case_str = "P([1,1] | [0.5,0.5], I(2))";
            if (Math.abs(prob_x - exp_prob_x) > 0.000001) {
                  console.log("Fail in test case " + case_str + "," +
                        "expected: [" + exp_prob_x + "], but got [" + prob_x + "]");
            } else {
                  console.log("Test case: " + case_str + " passed, "+
                        "expected: [" + exp_prob_x + "], and got [" + prob_x + "]");
            }
            
            // when mu[i] = mu[j] && sigma[i] = sigma[j], then gamma_z = pi.
            var X = [[0, 1], [1, 0], [1, 1], [0, 0], [4, 1]];
            var PI =[[0.5, 0.5], [0.3, 0.7], [0.1, 0.9], [0.4, 0.6], [0.2, 0.8]];
            for (var i = 0; i < X.length; ++i) {
                  X[i] = $V(X[i]);
                  PI[i] = $V(PI[i]);
            }
            for (var i = 0; i < PI.length; ++i) {
                  pi = PI[i];
                var gamma_z = e_step(X);
                for (var n = 0; n < gamma_z.length; ++n) {
                    if (!(gamma_z[n].elements[0] == PI[i].elements[0] && 
                            gamma_z[n].elements[1] == PI[i].elements[1])) {
                          console.log("Expected " + PI[i].elements + " but got: " + gamma_z[n].elements);
                    }
                }
            }

            pi = $V([0.5, 0.5]);
            mu[0] = $V([1, 0]);
            mu[1] = $V([2, 1]);
            var X1 = [$V([0, 1])];
            var gamma_z_1 = e_step(X1);
            var exp_gamma_z_1 = [Math.exp(-1)/(Math.exp(-1) + Math.exp(-2)), Math.exp(-2)/(Math.exp(-1) + Math.exp(-2))];
            if (!(approx_eq(gamma_z_1[0].elements[0], exp_gamma_z_1[0], 0.00001) &&
                    approx_eq(gamma_z_1[0].elements[1], exp_gamma_z_1[1], 0.00001))) {
                  console.log("Expected " + exp_gamma_z_1 + " but got: " + gamma_z_1[0].elements);
            }
        }
        
        return {
            Init: init,
            SetMu: function(m) {
                for (var k = 0; k < K; ++k) {
                    mu[k] = $V(m[k]);
                }
            },
            Mu: function() {
                var m_ = [];
                for (var k = 0; k < K; ++k) {
                  m_[k] = mu[k].elements;
                }
                return m_;
            },
            Sigma: function() {
                var sigma_ = [];
                for (var k = 0; k < K; ++k) {
                  sigma_[k] = sigma[k].elements;
                }
                return sigma_;
            },
            ProbX: function(x) {
                var x_ = $V(x);
                return probX(x_);
            },
            ProbZGivenX: function(x) {
                var x_ = $V(x);
                var z_ = probZGivenX(x_);
                return z_.elements;
            },
            Train: function(X, spec) {
                var X_ = [];
                for (var i = 0; i < X.length; ++i) {
                    X_.push($V(X[i]));
                }
                return train(X_, spec);
            },
            UnitTest: unit_test,
        }
    }
    
    // [MainScriptEnds]
    
    var TestGaussianMixtureModel = function() {
        var gmm = new GaussianMixtureModel();
        gmm.UnitTest();
    }

    var plot_data = function(container, duration_interval_data, initial_mu, final_mu) {
        chart = new Highcharts.Chart({
            chart: {
                renderTo: container,
                type: 'scatter',
                zoomType: 'xy'
            },
            title: {
                text: 'Old Faithful Geysey Eruption Interval Vs Duration'
            },
            xAxis: {
                title: {
                    enabled: true,
                    text: 'Duration (min)'
                },
            },
            yAxis: {
                title: {
                    text: 'Interval (min)'
                }
            },
            tooltip: {
                formatter: function() {
                        return this.x +' , '+ this.y;
                }
            },
            series: [{
                name: 'Duration Vs Interval',
                data: duration_interval_data,
            }, {
                name: 'Initial Mu',
                data: initial_mu,
                color: 'red',
            }, {
                name: 'Final Mu',
                data: final_mu,
                color: 'green',
            }]
        });
    };
    
    var plot_hist = function(container, data) {
        chart = new Highcharts.Chart({
            chart: {
                renderTo: container,
                zoomType: 'xy'
            },
            title: {
                text: 'Histogram of ' + data.title
            },
            xAxis: {
                title: {
                    enabled: true,
                    text: data.title,
                },
                reversed: false,
                opposite: false,
                max: data.max_x,
                min: data.min_x,
            },
            yAxis: [{
                title: {
                    text: 'Frequency'
                },
                reversed: data.is_reverse,
                opposite: false,
            },{
                title: {
                   text: 'PDF value'
                },
                min: 0.0,
                opposite: true,
            }],
            tooltip: {
                formatter: function() {
                        return this.x +' , '+ this.y;
                }
            },
            series: [{
                name: data.title,
                data: data.duration_data,
                type: data.plot_type,
            }, {
                name: 'Gaussian 1',
                data: data.gaussian_1,
                type: 'line',
                yAxis: 1,
            }, {
                name: 'Gaussian_2',
                data: data.gaussian_2,
                type: 'line',
                yAxis: 1,
            }]
        });
    };
    
    
    var plot_projection = function(series_data, proj_elmnt, mean, stdev, 
        plot_div, plot_type, reverse, min_x, max_x, title, num_buckets) {
        // 1-Dimensional Projection
        var min_v = min_x;
        var max_v = max_x;
        var bucket_size = (max_v - min_v) / num_buckets;
        var hist = [];
        for (var i = 0; i < num_buckets+1; ++i) {
          hist[i] = 0;
        }
        var t = 0;
        for (var i = 0; i < series_data.length; ++i) {
          var cur_duration = series_data[i][proj_elmnt];
          var bucket_index = Math.round((cur_duration - min_v) / bucket_size);
          hist[bucket_index] += 1;
          t += 1;
        }
        var hist_plot_data = [];
        var s = 0;
        for (var i = 0; i < hist.length; ++i) {
          var d = min_v + i * bucket_size;
          hist_plot_data[i] = {x: d, y: hist[i] / series_data.length};
          s += hist[i] / series_data.length;
        }
        var fitted_col_data = [];
        var norm_1 = $RND.gaussian_pdf(mean[0], stdev[0]);
        var norm_2 = $RND.gaussian_pdf(mean[1], stdev[1]);
        var dist_1 = [];
        var dist_2 = [];
        for (var i = 0; i < hist.length*5; ++i) {
          var d = min_v + i * bucket_size / 5;
          dist_1[i] = {x: d, y: norm_1(d)};
          dist_2[i] = {x: d, y: norm_2(d)};
        }
        plot_hist(plot_div, {
          duration_data: hist_plot_data,
          gaussian_1: dist_1,
          gaussian_2: dist_2,
          plot_type: plot_type,
          is_reversed: reverse,
          min_x: min_x,
          max_x: max_x,
          title: title,
        });
    }
    
    $(document).ready(function(){
           var old_faithful = DataSet.old_faithful;
           var duration_index = old_faithful.fieldnames["Duration"];
           var interval_index = old_faithful.fieldnames["Interval"];

           var duration_interval_data = [];
           for (var i = 0; i < old_faithful.data.length; ++i) {
               var duration = old_faithful.data[i][duration_index];
               var interval = old_faithful.data[i][interval_index];
               if (!isNaN(duration) && !isNaN(interval)) {
                  duration_interval_data.push([duration, interval]);
               }
           }
           TestGaussianMixtureModel();
            
           var duration_sum = 0.0;
           var interval_sum = 0.0;
           for (var i = 0; i < duration_interval_data.length; ++i) {
               duration_sum += duration_interval_data[i][0];
               interval_sum += duration_interval_data[i][1];
           }
           var average = [duration_sum / duration_interval_data.length, 
                          interval_sum / duration_interval_data.length];
            
           var train_spec = {
               min_increment: 0.001,
               max_steps: 100,
           };
           var gmm = new GaussianMixtureModel();
           gmm.Init(2, 2);
           var initial_mu = [[average[0]*0.7, average[1]*1.3], [average[0]*1.3, average[1]*0.7]];
           gmm.SetMu(initial_mu);
           gmm.Train(duration_interval_data, train_spec);
            
            // Plot the result.
           var duration_interval_data = [];
           for (var i = 0; i < old_faithful.data.length; ++i) {
               var duration = old_faithful.data[i][duration_index];
               var interval = old_faithful.data[i][interval_index];
               if (!isNaN(duration) && !isNaN(interval)) {
                   var z = gmm.ProbZGivenX([duration,interval]);
                   var r = Math.round(255 * z[0]);
                   var b = Math.round(255 * z[1]);
                   duration_interval_data.push({
                       x: duration, 
                       y: interval, 
                       color: 'rgb('+r+', 170, '+b+')',
                       z: z,
                   });
               }
           }
           plot_data('graph', duration_interval_data, initial_mu, gmm.Mu());

           var fitted_mean = gmm.Mu();
           var fitted_var = gmm.Sigma();
           plot_projection(duration_interval_data, 
               'y', 
               [fitted_mean[0][1], fitted_mean[1][1]], 
               [Math.sqrt(fitted_var[0][1][1]), Math.sqrt(fitted_var[1][1][1])],
               'histogram_interval',
               'bar',
               true,
               40, 120,
               "Interval",
               60);
           
           plot_projection(duration_interval_data, 
               'x', 
               [fitted_mean[0][0], fitted_mean[1][0]], 
               [Math.sqrt(fitted_var[0][0][0]), Math.sqrt(fitted_var[1][0][0])],
               'histogram_duration',
               'column',
               false,
               1.5, 5,
               "Duration",
               50);
           RenderCodeSection('main_script', 'code');
    });
    </script>
</head>
<body>
    <h1>Gaussian Mixture Model</h1>
    <p>by Weidong Liang</p>
    <p>Beijing, 2014.09</p>
    <hr/>
    
    <h2>Introduction</h2>
    <p>Mixture of Gaussians provides a way for soft clustering of data points into groups.
    It is a generative model of K Gaussian distribution of the form:
    $$
        p(x) = \sum_{k=1}^{K}{\pi_k N(x \mid \mu_k, \Sigma_k)}
    $$
    The probablity of data data point x belonging to group k is given by:
    $$
        p(z_k = 1 \mid x)  = N(x \mid \mu_k, \Sigma_k)
    $$
    where N is the multi-variable Gaussian:
    $$
        N(x \mid \mu_k, \Sigma_k) = \frac{1}{(2 \pi)^{\frac{D}{2}} \sqrt{\vert \Sigma \vert} } 
            \exp{\left({-\frac{1}{2}} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k)\right)}
    $$
    The marginal probability over z is specified in terms of the mixing coefficients \( \pi_k \)
    $$
        p(z_k = 1) = \pi_k, 0 \le \pi_k \le 1, \sum_{k=1}^{K}{\pi_k} = 1
    $$
    Using 1-of-K encoding for z, we have:
    $$
        p(z) = \prod_{k=1}^K{\pi_k^{z_k}}
    $$
    $$
        p(x \mid z) = \prod_{k=1}^K{ N(x \mid \mu_k, \Sigma_k) ^ {z_k} }
    $$
    </p>
    <h3>Training GMM using the Expectation-Maximization Algorithm</h3>
    <p>For the E-step, we have:
    $$
        p(z_k = 1 \mid x_n) = \gamma(z_{nk}) = \frac{p(z_k = 1, x_n)}{\sum_{j=1}^K{p(z_j = 1, x_n)}}
            = \frac{\pi_k N(x_n \mid \mu_k, \Sigma_k)}{ \sum_{j=1}^K{\pi_j N(x_n \mid \mu_j, \Sigma_j)} }
    $$
    </p>
    <p>For the M-step, we have to maximize:
    $$
        \Theta = argmax_{\Theta_i}  Q(\Theta_i, \Theta^{old})  
            = argmax_{\Theta_i} \sum_Z{p(Z \mid X, \Theta^{old}) \ln p(X, Z \mid \Theta_i)}
    $$
    In this case: 
    $$ Q(\Theta, \Theta^{old}) = \sum_{n=1}^N{ \sum_{k=1}^K{ 
        p(z_k = 1 | x_n, \Theta^{old}) \ln p(x_n, z_k = 1 \mid \Theta)} }
        =  \sum_{n=1}^N{ \sum_{k=1}^K{ \gamma(z_{nk}) \ln \pi_k N(x_n \mid \mu_k, \Sigma_k) } }
        =  \sum_{n=1}^N{ \sum_{k=1}^K{
            \gamma(z_{nk}) \left( \ln \pi_k + \ln N(x_n \mid \mu_k, \Sigma_k)  \right)  
        }}
    $$
    For \( \pi_k \), we need to maximize \( Q(\Theta, \Theta^{old}) \) subject to constraint \( \sum_{k=1}^K(\pi_k) = 1 \):
    $$
       \frac{\partial}{\partial \pi_k} \left[ Q(\Theta, \Theta^{old}) + \lambda_0 \left( 1 - \sum_{k=1}^K(\pi_k) \right) \right]
        = \sum_{n=1}^N{\gamma(z_{nk})\frac{1}{\pi_k}} - \lambda_0 = 0 \\
        =>  \pi_k = \frac{\sum_{n=1}^N{\gamma(z_{nk})}}{ \sum_{n=1}^N \sum_{k=1}^K{\gamma(z_{nk})} } = \frac{N_k}{N}
    $$
    Similarly for \( \mu_k \) and \( \Sigma_k \), we have:
    $$
        \frac{\partial}{\partial \mu_k} \left[ Q(\Theta, \Theta^{old}) + \lambda_0 \left( 1 - \sum_{k=1}^K(\pi_k) \right) \right] = 0 \\
        =>  \mu_k = \frac{1}{N_k}{\sum_{n=1}^N{ \gamma(z_{nk}) x_n }}
    $$
    $$
        \frac{\partial}{\partial \Sigma_k} \left[ Q(\Theta, \Theta^{old}) + \lambda_0 \left( 1 - \sum_{k=1}^K(\pi_k) \right) \right] = 0 \\
        => \Sigma_k = \frac{1}{N_k}{\gamma(z_{nk})(x_n - \mu_k^{new})(x_n - \mu_k^{new})^T }
    $$
    </p>
    <hr/>
    
    <h2>Simulation</h2>
    <p>
    </p>
    <table>
        <tr>
            <td><div id="graph" style="width:600px;height:600px"></div></td>
            <td><div id="histogram_interval" style="width:600px;height:600px"></div></td>
        </tr>
        <tr>
            <td><div id="histogram_duration" style="width:600px;height:600px"></div></td>
        </tr>
    </table>
    <hr/>
    
    <h2>Algorithm</h2>
    <p>
        <ol>
            <li>Initialize paramters \( \mu_k \), \( \Sigma_k \), and \( \pi_k \).</li>
            <li>E-step, evaluate \( p(z \mid X, \Theta) \)
            $$
                \gamma(z_{nk}) = \frac{\pi_k N (x_n \mid \mu_k, \Sigma_k)}
                    {\sum_{j=1}^K{\pi_j N(x_n \mid \mu_j, \Sigma_j)}}
            $$
            </li>
            <li>M-step, maximize \( \sum_{Z}{ p(Z \mid X, \Theta^{old}) \ln{ p(X, Z \mid \Theta) } } \)
            $$
                N_k = \sum_{n=1}^{N}{\gamma(z_{nk})}
            $$
            $$
                \mu_k^{new} = \frac{1}{N_k}{\sum_{n=1}^N{ \gamma(z_{nk}) x_n }}
            $$
            $$
                \Sigma_k^{new} = \frac{1}{N_k}{\gamma(z_{nk})(x_n - \mu_k^{new})(x_n - \mu_k^{new})^T }
            $$
            $$
                \pi_k^{new} = \frac{N_k}{N}
            $$
            </li>
            <li>Evaluate log likelihood and check for convergence:
            $$
                \ln p(X \mid \mu, \Sigma, \pi) = 
                    \sum_{n=1}^N{\ln\{\sum_{k = 1}^K{\pi_k N(x_n \mid \mu_k, \Sigma_k)}\}}
            $$
            </li>
        </ol>
    </p>
    <hr/>
    
    <h2>Implementation</h2>
    <div id='code'></div>
    <hr/>
    
    <h2>Note</h2>
    <h3>The EM Algorithm</h3>
    <p>For models involving hidden variables, training involves marginalizing over Z, i.e. 
    \( p(X) = \sum_Z{p(X, Z)} \), and evaluation of the summation is usually not possible
    due to the exponential number of terms needed to evaluated. The EM algorithm avoids this
    short-comming by evaluating \( p(X, Z) \) and \( p(Z) \) instead of \( p(X) \).
    </p>
    </p>Assuming that we have to maximize \( p(X \mid \Theta) = \sum_Z{ p(X, Z \mid \Theta)} \),
    we have the following derivation:
    $$
        p(X, Z \mid \Theta) = p(Z \mid X, \Theta) * p(X \mid \Theta)
    $$
    $$
       \ln p(X \mid \Theta) = \ln p(X, Z \mid \Theta) - \ln p(Z \mid X, \Theta)
    $$
    $$
       \ln p(X \mid \Theta) = \ln p(X, Z \mid \Theta) - \ln p(Z \mid X, \Theta) + \ln q(Z) - \ln q(Z)
    $$
    $$
       \ln p(X \mid \Theta) = \ln \frac{p(X, Z \mid \Theta)}{q(Z)} - \ln \frac{p(Z \mid X, \Theta)}{q(Z)}
    $$
    $$
       \sum_Z{q(Z) \ln p(X \mid \Theta)} = \sum_Z{q(Z) \ln \frac{p(X, Z \mid \Theta)}{q(Z)} } 
            - \sum_Z{q(Z) \ln \frac{p(Z \mid X, \Theta)}{q(Z)} }
    $$
    $$
       \left( \sum_Z{q(Z)} \right) \left( \ln p(X \mid \Theta) \right) = \sum_Z{q(Z) \ln \frac{p(X, Z \mid \Theta)}{q(Z)} } 
            - \sum_Z{q(Z) \ln \frac{p(Z \mid X, \Theta)}{q(Z)} }
    $$
    $$
        \ln p(X \mid \Theta) = \sum_Z{q(Z) \ln \frac{p(X, Z \mid \Theta)}{q(Z)} } 
            - \sum_Z{q(Z) \ln \frac{p(Z \mid X, \Theta)}{q(Z)} }
    $$
    $$
        \ln p(X \mid \Theta) = L(q, \Theta) + KL(p || q)
    $$
    where \( KL(p || q) \) is the Kullback-Leibler divergence.
    </p>
    <p>Since \( KL(p || q)  \ge 0 \), therefore \( L(q, \Theta) \le \ln p(X \mid \Theta) \),
    i.e. \( L(q, \Theta)  \) is the lower bound of \( \ln p(X \mid \Theta) \).
    </p>
    <p>
    In the Expectation Step, the lower bound \( L(q, \Theta) \) is maximized with respect to \( q(Z) \)
    while holding \( \Theta \) fixed, which it is at its maximum when \( KL(p || q) = 0 \), 
    i.e. \( q(Z) = p(Z \mid X, \Theta) \).
    </p>
    <p>
    In the Maximization Step, the lower bound \( L(q, \Theta) \) is maximized with respect to \( \Theta \) 
    while holding \( q(Z) \) fixed. Since \( q(Z) = p(Z \mid, X, \Theta^{old}) \) from the E-step, we have:
    $$
        L(q, \Theta) = \sum_Z{p(Z \mid X, \Theta^{old}) \ln p(X, Z \mid \Theta)}
            - \sum_Z{p(Z \mid X, \Theta^{old}) \ln p(Z \mid X, \Theta^{old})}
            =  \sum_Z{p(Z \mid X, \Theta^{old}) \ln p(X, Z \mid \Theta)} + const
    $$
    Therefore, the M-Step is equivalent to maximize:
    $$
        Q(\Theta, \Theta^{old}) = \sum_Z{p(Z \mid X, \Theta^{old}) \ln p(X, Z \mid \Theta)} 
    $$
    Note that maximization of \( Q(\Theta, \Theta^{old}) \) can be easier than direct maximization of \( \ln p(X) \)
    due to the presence of the joint distribution in the logarithm if the it contains a member of
    the exponential family or a product of such.
    </p>
    <p>Summing up, the EM algorithms alternate between the evaluation of 
     \( q(Z) = p(Z \mid X, \Theta^{old}) \) and maximization of 
     \( Q(\Theta, \Theta^{old}) = \sum_Z{p(Z \mid X, \Theta^{old}) \ln p(X, Z \mid \Theta)} \)
     till convergence to achive model training.
    </p>
    <hr/>
    
    <h2>Reference</h2>
    <ul>
        <li>Bishop: Pattern Recognition and Machine Learning</li>
    </ul>
    <hr/>
    
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'weidongtomlgithubio'; // required: replace example with your forum shortname
        // var disqus_url = 'http://weidongtoml.github.io/sampling/index.html';
        // var disqus_developer = 1;
        var disqus_identifier = '';
        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</body>
</html>
    